======================================
TRANSFORMER EXPERIMENTS - OFFENSIVE LANGUAGE DETECTION
======================================

Working directory: /gaueko1/users/mmartin/offensEval-project
Python: /gaueko1/users/mmartin/offensEval-project/venv/bin/python

Checking GPU...
GPU Available: True
GPU: Tesla V100-PCIE-32GB

======================================
BERT-BASE EXPERIMENTS
======================================
[1/6] BERT + Raw

============================================================
TRANSFORMER FINE-TUNING FOR OFFENSIVE LANGUAGE DETECTION
============================================================
Model: bert-base-uncased
Display name: BERT-BASE-UNCASED
Preprocessing: raw
Device: cuda
Batch size: 16
Learning rate: 2e-05
Max length: 128

============================================================
LOADING DATA
============================================================
Train: 12240 examples
Dev: 1000 examples
Test: 860 examples
Class distribution (train): {'NOT': np.int64(8192), 'OFF': np.int64(4048)}

============================================================
LOADING MODEL AND TOKENIZER
============================================================
Tokenizer loaded: BertTokenizerFast
Model parameters: 109,483,778
Total training steps: 3060
Warmup steps: 306

============================================================
TRAINING
============================================================

Epoch 1/4
Epoch  1/4 | Loss: 0.5060 | Dev F1: 0.7836 | Dev Acc: 0.8070
  → New best model saved (F1: 0.7836)

Epoch 2/4
Epoch  2/4 | Loss: 0.3642 | Dev F1: 0.7902 | Dev Acc: 0.8110
  → New best model saved (F1: 0.7902)

Epoch 3/4
Epoch  3/4 | Loss: 0.2491 | Dev F1: 0.7816 | Dev Acc: 0.8040

Epoch 4/4
Epoch  4/4 | Loss: 0.1627 | Dev F1: 0.7797 | Dev Acc: 0.7980

Early stopping at epoch 4

Loaded best model from results/best_bert-base-uncased_raw.pt

============================================================
FINAL EVALUATION
============================================================

Development Set:
  Accuracy:  0.8110
  Macro F1:  0.7902
  F1 (OFF):  0.7241
  F1 (NOT):  0.8563

              precision    recall  f1-score   support

         NOT     0.8441    0.8688    0.8563       648
         OFF     0.7447    0.7045    0.7241       352

    accuracy                         0.8110      1000
   macro avg     0.7944    0.7867    0.7902      1000
weighted avg     0.8091    0.8110    0.8097      1000


Test Set:
  Accuracy:  0.8465
  Macro F1:  0.8019
  F1 (OFF):  0.7080
  F1 (NOT):  0.8959

              precision    recall  f1-score   support

         NOT     0.8765    0.9161    0.8959       620
         OFF     0.7547    0.6667    0.7080       240

    accuracy                         0.8465       860
   macro avg     0.8156    0.7914    0.8019       860
weighted avg     0.8425    0.8465    0.8435       860

Confusion matrix saved to confusion_matrix_bert-base-uncased_dev.png
Confusion matrix saved to confusion_matrix_bert-base-uncased_test.png

============================================================
ERROR ANALYSIS: bert-base-uncased (raw)
============================================================

Total errors: 132/860 (15.3%)

Confusion patterns:
  OFF → NOT (false negatives): 80 (60.6%)
  NOT → OFF (false positives): 52 (39.4%)

High-confidence errors (conf > 0.8): 71

Error rate by text length:
  Very short (≤5): 11.4% (44 tweets)
  Short (6-10): 18.8% (101 tweets)
  Medium (11-20): 11.4% (246 tweets)
  Long (>20): 17.1% (469 tweets)
Detailed error analysis saved to: results//error_analysis_bert-base-uncased_raw.txt

============================================================
SAVING RESULTS
============================================================
Results saved to: results/transformer_bert-base-uncased_raw_results.csv

============================================================
EXPERIMENT COMPLETED
============================================================
Best dev F1:  0.7902
Test F1:      0.8019
Test Acc:     0.8465

[2/6] BERT + Clean

============================================================
TRANSFORMER FINE-TUNING FOR OFFENSIVE LANGUAGE DETECTION
============================================================
Model: bert-base-uncased
Display name: BERT-BASE-UNCASED
Preprocessing: clean
Device: cuda
Batch size: 16
Learning rate: 2e-05
Max length: 128

============================================================
LOADING DATA
============================================================
Train: 12240 examples
Dev: 1000 examples
Test: 860 examples
Class distribution (train): {'NOT': np.int64(8192), 'OFF': np.int64(4048)}

============================================================
LOADING MODEL AND TOKENIZER
============================================================
Tokenizer loaded: BertTokenizerFast
Model parameters: 109,483,778
Total training steps: 3060
Warmup steps: 306

============================================================
TRAINING
============================================================

Epoch 1/4
Epoch  1/4 | Loss: 0.5107 | Dev F1: 0.7932 | Dev Acc: 0.8190
  → New best model saved (F1: 0.7932)

Epoch 2/4
Epoch  2/4 | Loss: 0.3652 | Dev F1: 0.7802 | Dev Acc: 0.8070

Epoch 3/4
Epoch  3/4 | Loss: 0.2458 | Dev F1: 0.7600 | Dev Acc: 0.7930

Early stopping at epoch 3

Loaded best model from results/best_bert-base-uncased_clean.pt

============================================================
FINAL EVALUATION
============================================================

Development Set:
  Accuracy:  0.8190
  Macro F1:  0.7932
  F1 (OFF):  0.7202
  F1 (NOT):  0.8662

              precision    recall  f1-score   support

         NOT     0.8312    0.9043    0.8662       648
         OFF     0.7898    0.6619    0.7202       352

    accuracy                         0.8190      1000
   macro avg     0.8105    0.7831    0.7932      1000
weighted avg     0.8166    0.8190    0.8148      1000


Test Set:
  Accuracy:  0.8547
  Macro F1:  0.8034
  F1 (OFF):  0.7031
  F1 (NOT):  0.9038

              precision    recall  f1-score   support

         NOT     0.8645    0.9468    0.9038       620
         OFF     0.8177    0.6167    0.7031       240

    accuracy                         0.8547       860
   macro avg     0.8411    0.7817    0.8034       860
weighted avg     0.8514    0.8547    0.8478       860

Confusion matrix saved to confusion_matrix_bert-base-uncased_dev.png
Confusion matrix saved to confusion_matrix_bert-base-uncased_test.png

============================================================
ERROR ANALYSIS: bert-base-uncased (clean)
============================================================

Total errors: 125/860 (14.5%)

Confusion patterns:
  OFF → NOT (false negatives): 92 (73.6%)
  NOT → OFF (false positives): 33 (26.4%)

High-confidence errors (conf > 0.8): 65

Error rate by text length:
  Very short (≤5): 15.4% (52 tweets)
  Short (6-10): 14.7% (102 tweets)
  Medium (11-20): 11.2% (251 tweets)
  Long (>20): 16.3% (455 tweets)
Detailed error analysis saved to: results//error_analysis_bert-base-uncased_clean.txt

============================================================
SAVING RESULTS
============================================================
Results saved to: results/transformer_bert-base-uncased_clean_results.csv

============================================================
EXPERIMENT COMPLETED
============================================================
Best dev F1:  0.7932
Test F1:      0.8034
Test Acc:     0.8547

[3/6] BERT + Aggressive

============================================================
TRANSFORMER FINE-TUNING FOR OFFENSIVE LANGUAGE DETECTION
============================================================
Model: bert-base-uncased
Display name: BERT-BASE-UNCASED
Preprocessing: aggressive
Device: cuda
Batch size: 16
Learning rate: 2e-05
Max length: 128

============================================================
LOADING DATA
============================================================
Train: 12240 examples
Dev: 1000 examples
Test: 860 examples
Class distribution (train): {'NOT': np.int64(8192), 'OFF': np.int64(4048)}

============================================================
LOADING MODEL AND TOKENIZER
============================================================
Tokenizer loaded: BertTokenizerFast
Model parameters: 109,483,778
Total training steps: 3060
Warmup steps: 306

============================================================
TRAINING
============================================================

Epoch 1/4
Epoch  1/4 | Loss: 0.5115 | Dev F1: 0.7800 | Dev Acc: 0.8010
  → New best model saved (F1: 0.7800)

Epoch 2/4
Epoch  2/4 | Loss: 0.3681 | Dev F1: 0.7872 | Dev Acc: 0.8070
  → New best model saved (F1: 0.7872)

Epoch 3/4
Epoch  3/4 | Loss: 0.2533 | Dev F1: 0.7823 | Dev Acc: 0.8060

Epoch 4/4
Epoch  4/4 | Loss: 0.1715 | Dev F1: 0.7722 | Dev Acc: 0.7910

Early stopping at epoch 4

Loaded best model from results/best_bert-base-uncased_aggressive.pt

============================================================
FINAL EVALUATION
============================================================

Development Set:
  Accuracy:  0.8070
  Macro F1:  0.7872
  F1 (OFF):  0.7223
  F1 (NOT):  0.8521

              precision    recall  f1-score   support

         NOT     0.8463    0.8580    0.8521       648
         OFF     0.7318    0.7131    0.7223       352

    accuracy                         0.8070      1000
   macro avg     0.7890    0.7855    0.7872      1000
weighted avg     0.8060    0.8070    0.8064      1000


Test Set:
  Accuracy:  0.8302
  Macro F1:  0.7840
  F1 (OFF):  0.6840
  F1 (NOT):  0.8839

              precision    recall  f1-score   support

         NOT     0.8715    0.8968    0.8839       620
         OFF     0.7117    0.6583    0.6840       240

    accuracy                         0.8302       860
   macro avg     0.7916    0.7776    0.7840       860
weighted avg     0.8269    0.8302    0.8281       860

Confusion matrix saved to confusion_matrix_bert-base-uncased_dev.png
Confusion matrix saved to confusion_matrix_bert-base-uncased_test.png

============================================================
ERROR ANALYSIS: bert-base-uncased (aggressive)
============================================================

Total errors: 146/860 (17.0%)

Confusion patterns:
  OFF → NOT (false negatives): 82 (56.2%)
  NOT → OFF (false positives): 64 (43.8%)

High-confidence errors (conf > 0.8): 58

Error rate by text length:
  Very short (≤5): 12.3% (57 tweets)
  Short (6-10): 12.5% (112 tweets)
  Medium (11-20): 13.7% (255 tweets)
  Long (>20): 20.6% (436 tweets)
Detailed error analysis saved to: results//error_analysis_bert-base-uncased_aggressive.txt

============================================================
SAVING RESULTS
============================================================
Results saved to: results/transformer_bert-base-uncased_aggressive_results.csv

============================================================
EXPERIMENT COMPLETED
============================================================
Best dev F1:  0.7872
Test F1:      0.7840
Test Acc:     0.8302

======================================
ROBERTA-BASE EXPERIMENTS
======================================
[4/6] RoBERTa + Raw

============================================================
TRANSFORMER FINE-TUNING FOR OFFENSIVE LANGUAGE DETECTION
============================================================
Model: roberta-base
Display name: ROBERTA-BASE
Preprocessing: raw
Device: cuda
Batch size: 16
Learning rate: 2e-05
Max length: 128

============================================================
LOADING DATA
============================================================
Train: 12240 examples
Dev: 1000 examples
Test: 860 examples
Class distribution (train): {'NOT': np.int64(8192), 'OFF': np.int64(4048)}

============================================================
LOADING MODEL AND TOKENIZER
============================================================
Tokenizer loaded: RobertaTokenizerFast
Model parameters: 124,647,170
Total training steps: 3060
Warmup steps: 306

============================================================
TRAINING
============================================================

Epoch 1/4
Epoch  1/4 | Loss: 0.5127 | Dev F1: 0.7504 | Dev Acc: 0.7930
  → New best model saved (F1: 0.7504)

Epoch 2/4
Epoch  2/4 | Loss: 0.3999 | Dev F1: 0.7830 | Dev Acc: 0.8000
  → New best model saved (F1: 0.7830)

Epoch 3/4
Epoch  3/4 | Loss: 0.3129 | Dev F1: 0.7793 | Dev Acc: 0.7940

Epoch 4/4
Epoch  4/4 | Loss: 0.2409 | Dev F1: 0.7772 | Dev Acc: 0.7950

Early stopping at epoch 4

Loaded best model from results/best_roberta-base_raw.pt

============================================================
FINAL EVALUATION
============================================================

Development Set:
  Accuracy:  0.8000
  Macro F1:  0.7830
  F1 (OFF):  0.7222
  F1 (NOT):  0.8438

              precision    recall  f1-score   support

         NOT     0.8544    0.8333    0.8438       648
         OFF     0.7065    0.7386    0.7222       352

    accuracy                         0.8000      1000
   macro avg     0.7805    0.7860    0.7830      1000
weighted avg     0.8024    0.8000    0.8010      1000


Test Set:
  Accuracy:  0.8419
  Macro F1:  0.7993
  F1 (OFF):  0.7069
  F1 (NOT):  0.8917

              precision    recall  f1-score   support

         NOT     0.8805    0.9032    0.8917       620
         OFF     0.7321    0.6833    0.7069       240

    accuracy                         0.8419       860
   macro avg     0.8063    0.7933    0.7993       860
weighted avg     0.8391    0.8419    0.8401       860

Confusion matrix saved to confusion_matrix_roberta-base_dev.png
Confusion matrix saved to confusion_matrix_roberta-base_test.png

============================================================
ERROR ANALYSIS: roberta-base (raw)
============================================================

Total errors: 136/860 (15.8%)

Confusion patterns:
  OFF → NOT (false negatives): 76 (55.9%)
  NOT → OFF (false positives): 60 (44.1%)

High-confidence errors (conf > 0.8): 57

Error rate by text length:
  Very short (≤5): 15.9% (44 tweets)
  Short (6-10): 15.8% (101 tweets)
  Medium (11-20): 13.8% (246 tweets)
  Long (>20): 16.8% (469 tweets)
Detailed error analysis saved to: results//error_analysis_roberta-base_raw.txt

============================================================
SAVING RESULTS
============================================================
Results saved to: results/transformer_roberta-base_raw_results.csv

============================================================
EXPERIMENT COMPLETED
============================================================
Best dev F1:  0.7830
Test F1:      0.7993
Test Acc:     0.8419

[5/6] RoBERTa + Clean

============================================================
TRANSFORMER FINE-TUNING FOR OFFENSIVE LANGUAGE DETECTION
============================================================
Model: roberta-base
Display name: ROBERTA-BASE
Preprocessing: clean
Device: cuda
Batch size: 16
Learning rate: 2e-05
Max length: 128

============================================================
LOADING DATA
============================================================
Train: 12240 examples
Dev: 1000 examples
Test: 860 examples
Class distribution (train): {'NOT': np.int64(8192), 'OFF': np.int64(4048)}

============================================================
LOADING MODEL AND TOKENIZER
============================================================
Tokenizer loaded: RobertaTokenizerFast
Model parameters: 124,647,170
Total training steps: 3060
Warmup steps: 306

============================================================
TRAINING
============================================================

Epoch 1/4
Epoch  1/4 | Loss: 0.5160 | Dev F1: 0.7450 | Dev Acc: 0.7900
  → New best model saved (F1: 0.7450)

Epoch 2/4
Epoch  2/4 | Loss: 0.3988 | Dev F1: 0.7717 | Dev Acc: 0.7870
  → New best model saved (F1: 0.7717)

Epoch 3/4
Epoch  3/4 | Loss: 0.3123 | Dev F1: 0.7857 | Dev Acc: 0.8000
  → New best model saved (F1: 0.7857)

Epoch 4/4
Epoch  4/4 | Loss: 0.2445 | Dev F1: 0.7829 | Dev Acc: 0.8010

Loaded best model from results/best_roberta-base_clean.pt

============================================================
FINAL EVALUATION
============================================================

Development Set:
  Accuracy:  0.8000
  Macro F1:  0.7857
  F1 (OFF):  0.7305
  F1 (NOT):  0.8410

              precision    recall  f1-score   support

         NOT     0.8672    0.8164    0.8410       648
         OFF     0.6949    0.7699    0.7305       352

    accuracy                         0.8000      1000
   macro avg     0.7810    0.7931    0.7857      1000
weighted avg     0.8065    0.8000    0.8021      1000


Test Set:
  Accuracy:  0.8384
  Macro F1:  0.8014
  F1 (OFF):  0.7157
  F1 (NOT):  0.8871

              precision    recall  f1-score   support

         NOT     0.8936    0.8806    0.8871       620
         OFF     0.7028    0.7292    0.7157       240

    accuracy                         0.8384       860
   macro avg     0.7982    0.8049    0.8014       860
weighted avg     0.8404    0.8384    0.8393       860

Confusion matrix saved to confusion_matrix_roberta-base_dev.png
Confusion matrix saved to confusion_matrix_roberta-base_test.png

============================================================
ERROR ANALYSIS: roberta-base (clean)
============================================================

Total errors: 139/860 (16.2%)

Confusion patterns:
  OFF → NOT (false negatives): 65 (46.8%)
  NOT → OFF (false positives): 74 (53.2%)

High-confidence errors (conf > 0.8): 83

Error rate by text length:
  Very short (≤5): 23.1% (52 tweets)
  Short (6-10): 11.8% (102 tweets)
  Medium (11-20): 15.5% (251 tweets)
  Long (>20): 16.7% (455 tweets)
Detailed error analysis saved to: results//error_analysis_roberta-base_clean.txt

============================================================
SAVING RESULTS
============================================================
Results saved to: results/transformer_roberta-base_clean_results.csv

============================================================
EXPERIMENT COMPLETED
============================================================
Best dev F1:  0.7857
Test F1:      0.8014
Test Acc:     0.8384

[6/6] RoBERTa + Aggressive

============================================================
TRANSFORMER FINE-TUNING FOR OFFENSIVE LANGUAGE DETECTION
============================================================
Model: roberta-base
Display name: ROBERTA-BASE
Preprocessing: aggressive
Device: cuda
Batch size: 16
Learning rate: 2e-05
Max length: 128

============================================================
LOADING DATA
============================================================
Train: 12240 examples
Dev: 1000 examples
Test: 860 examples
Class distribution (train): {'NOT': np.int64(8192), 'OFF': np.int64(4048)}

============================================================
LOADING MODEL AND TOKENIZER
============================================================
Tokenizer loaded: RobertaTokenizerFast
Model parameters: 124,647,170
Total training steps: 3060
Warmup steps: 306

============================================================
TRAINING
============================================================

Epoch 1/4
Epoch  1/4 | Loss: 0.5087 | Dev F1: 0.7460 | Dev Acc: 0.7910
  → New best model saved (F1: 0.7460)

Epoch 2/4
Epoch  2/4 | Loss: 0.4023 | Dev F1: 0.7803 | Dev Acc: 0.8050
  → New best model saved (F1: 0.7803)

Epoch 3/4
Epoch  3/4 | Loss: 0.3201 | Dev F1: 0.7746 | Dev Acc: 0.7920

Epoch 4/4
Epoch  4/4 | Loss: 0.2563 | Dev F1: 0.7663 | Dev Acc: 0.7850

Early stopping at epoch 4

Loaded best model from results/best_roberta-base_aggressive.pt

============================================================
FINAL EVALUATION
============================================================

Development Set:
  Accuracy:  0.8050
  Macro F1:  0.7803
  F1 (OFF):  0.7068
  F1 (NOT):  0.8539

              precision    recall  f1-score   support

         NOT     0.8297    0.8796    0.8539       648
         OFF     0.7508    0.6676    0.7068       352

    accuracy                         0.8050      1000
   macro avg     0.7902    0.7736    0.7803      1000
weighted avg     0.8019    0.8050    0.8021      1000


Test Set:
  Accuracy:  0.8419
  Macro F1:  0.7898
  F1 (OFF):  0.6852
  F1 (NOT):  0.8944

              precision    recall  f1-score   support

         NOT     0.8623    0.9290    0.8944       620
         OFF     0.7708    0.6167    0.6852       240

    accuracy                         0.8419       860
   macro avg     0.8166    0.7728    0.7898       860
weighted avg     0.8368    0.8419    0.8360       860

Confusion matrix saved to confusion_matrix_roberta-base_dev.png
Confusion matrix saved to confusion_matrix_roberta-base_test.png

============================================================
ERROR ANALYSIS: roberta-base (aggressive)
============================================================

Total errors: 136/860 (15.8%)

Confusion patterns:
  OFF → NOT (false negatives): 92 (67.6%)
  NOT → OFF (false positives): 44 (32.4%)

High-confidence errors (conf > 0.8): 55

Error rate by text length:
  Very short (≤5): 14.0% (57 tweets)
  Short (6-10): 11.6% (112 tweets)
  Medium (11-20): 14.9% (255 tweets)
  Long (>20): 17.7% (436 tweets)
Detailed error analysis saved to: results//error_analysis_roberta-base_aggressive.txt

============================================================
SAVING RESULTS
============================================================
Results saved to: results/transformer_roberta-base_aggressive_results.csv

============================================================
EXPERIMENT COMPLETED
============================================================
Best dev F1:  0.7803
Test F1:      0.7898
Test Acc:     0.8419

======================================
ALL EXPERIMENTS COMPLETED
======================================

Results saved in results/
-rw-r--r-- 1 mmartin jirxuxen 176 Oct 27 21:13 results/transformer_bert-base-uncased_aggressive_results.csv
-rw-r--r-- 1 mmartin jirxuxen 171 Oct 27 21:06 results/transformer_bert-base-uncased_clean_results.csv
-rw-r--r-- 1 mmartin jirxuxen 170 Oct 27 21:01 results/transformer_bert-base-uncased_raw_results.csv
-rw-r--r-- 1 mmartin jirxuxen 171 Oct 27 21:35 results/transformer_roberta-base_aggressive_results.csv
-rw-r--r-- 1 mmartin jirxuxen 167 Oct 27 21:28 results/transformer_roberta-base_clean_results.csv
-rw-r--r-- 1 mmartin jirxuxen 163 Oct 27 21:21 results/transformer_roberta-base_raw_results.csv

✅ DONE!
